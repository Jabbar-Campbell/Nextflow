
// ##########################   NF-CORE PIPELINES ##############################################
# NF-core modules come containerized
# you can take snippets and tools of things people have already written
# each will have a docs file that describes what it does in addition to required
# parameters




// pull a pipeline as demonstration
// very similiar to pulling a repo on github
// the repo will have .yml for spinning up containers
// .nf files with the inputs outputs scripts processes and workflows
$ nextflow pull nf-core/demo
$ nextflow list

// nf-core/demo takes a samplesheet that contains paths to fastq files as an input and will produce 
// a variety of logs and reports:
// this pipeline consists of three processes:
(FASTQC): Read QC
(FASTP): Adapter and quality trimming
(MULTIQC): Present QC for raw reads

// the four output folders will be....

// results/
// fastqc/
// *_fastqc.html: FastQC report containing quality metrics.
// *_fastqc.zip: Zip archive containing the FastQC report, tab-delimited data file and plot images.

// fastp/
// *.fastp.html: Trimming report in html format.
// *.fastp.json: Trimming report in json format.
// *.fastp.log: Trimming log file.
// *.fastq.gz: If --save_trimmed

// multiqc/
// multiqc_report.html: a standalone HTML file that can be viewed in your web browser.
// multiqc_data/: directory containing parsed statistics from the different tools used in the pipeline.
// multiqc_plots/: directory containing static images from the report in various formats.

// pipeline_info/
// Reports generated by Nextflow
// Reports generated by nf-core
// Parameters file


// we can create our own input file and add necessary data according to the docs
// header line, and for each sample, an id and the complete paths to the paired-end reads:
$ code samplesheet.csv

sample,fastq_1,fastq_2
gut,/workspace/gitpod/nf-customize/data/gut_1.fq.gz,/workspace/gitpod/nf-customize/data/gut_2.fq.gz
liver,/workspace/gitpod/nf-customize/data/liver_1.fq.gz,/workspace/gitpod/nf-customize/data/liver_2.fq.gz
lung,/workspace/gitpod/nf-customize/data/lung_1.fq.gz,/workspace/gitpod/nf-customize/data/lung_2.fq.gz

//  with the necessary parameters --input and --output we can excute the pipeline
nextflow run nf-core/demo -r dev --input samplesheet.csv --outdir results -profile singularity



// ##############################  CONGFIG FILES ################################################

// THE MORE PARAMETERS AND CONFIGURATIONS YOU CAN MOVE INTO CONFIG  and JSON FILES THE LESS WE HAVE TO 
// CHANGE THE .NF FILES THEMSELVES. ESPECIALLY IF THOSE PARAMETERS HAVE MEMORY AND RESOURCE SETTINGS
// THERE SHOULD BE MOSTLY BOOLEAN IN THESE FILES. IN NEXTFLOW THERE ARE MANY CONFIG FILES. THAT  HOUSE
// CONFIGURATION SETTINGS  SUCH AS READ WRITE PRVILAGES RESOURCE SETTING and PROFILES . CHECK THE ONE OF THE
// CONFIG FILE IN THE REPO CALLED nextflow.config TO SEE WHAT OTHER PROFILES MIGHT BE AVAILABLE 
// SUCH AS -profile docker
nextflow run nf-core/demo -r dev --input samplesheet.csv --outdir results -profile docker




// if you wanted to make your own config you would create one with the following
// then use -c instead of -profile
code mycustomconfig.config

singularity {
    enabled    = true
    autoMounts = true
}

// run with our custom made config file
nextflow run nf-core/demo -r dev -params-file mycustomparams.json -c mycustomconfig.config

// you can also use a test profile stored in the conf folder of the repo
// this will give you confidence that the pipeline runs
nextflow run nf-core/demo -r dev -profile test,singularity --outdir results

// in the config file there are other profiles in case you might want to run
// a given process with different parameters, for example maybe we want to trim
// during the fast qc step. 
nextflow run nf-core/demo -r dev --input samplesheet.csv --outdir results -profile singularity,profile1




// ###################################  PARAMETER FILES ###############################################
// in Nextflow parameteres can be stored and read  from a .json file
// saved as mycustomparams.json
// we can then execute nextflow without typing any parameters using  -params-file 
// nextflow has a GUI for entering parameters as well
{
"input": "/workspace/gitpod/nf-customize/samplesheet.csv",
"outdir": "results_customparams"
}

nextflow run nf-core/demo -r dev -profile singularity -params-file mycustomparams.json


// make  a pipepine of your own by creating a basic conserved pipeline framework 
// .git, readme file, configs file, CI file, will be created as follows
// you just need to add a repository and push it to github 
// in nf-core/demo pipeline there where three modules (FASTQC) (FASTP):(MULTIQC): 
// each having its own main.nf 
// nf.core is specific for the environment 
//   
// modules/
// ├── local
//│   └── <toolname> 
//│   │   └── main.nf
//│   .
//│
//└── nf-core
//    ├── <toolname>
//    │   ├── environment.yml
//    │   ├── main.nf
//    │   ├── meta.yml
//    │   └── tests
//    │       ├── main.nf.test
//    │       ├── main.nf.test.snap
//    │       └── tags.yml
//    .

nf-core create

// our newly created pipeline called "nf-core-myfirstpipeline" 
// by default  comes with sequencing tools and processes.... 
include { FASTQC                 } from '../modules/nf-core/fastqc/main'
include { MULTIQC                } from '../modules/nf-core/multiqc/main'
include { paramsSummaryMap       } from 'plugin/nf-validation'
include { paramsSummaryMultiqc   } from '../subworkflows/nf-core/utils_nfcore_pipeline'
include { softwareVersionsToYAML } from '../subworkflows/nf-core/utils_nfcore_pipeline'
include { methodsDescriptionText } from '../subworkflows/local/utils_nfcore_myfirstpipeline_pipeline'
workflow{...}

// like the demo  we can submit test data in a docker container
// feel free to add and remove features 
nextflow run nf-core-myfirstpipeline -profile test,docker --outdir results



// base.config is a file for customizing resoures allocation using labels
// add these labels to the main script to allocate the resource



//There might be one already for image recognition :) 
// but if we wanted to process 1000 images we can make a config file with container and  resources settings
// and reference those labels in each process. Save the Python file as an executable in a bin folder
// https://training.nextflow.io/basic_training/executors/#configuration-profiles


process myProcess1 {
    container 'my-docker-image-one'
    resourceLabels: "Label-1" = "myValue-1"
    input: "batch-1"
    output:
    script:
    // Instruct python to run  set number of files using the label  or container for those resources
}

process myProcess2 {
    container 'my-docker-image-two'
    resourceLabels: "myLabel-2" = "myValue-2"
    input: "batch-2"
    output:
    script:
    // Instruct python to run  another set of files using the label or containerfor those resources
}

process myProcess3 {
    container 'my-docker-image-three'
    resourceLabels: "myLabel-3" = "myValue-3"
    input: "batch-3"
    output:
    script:
    // Instruct python to run  another set of files using the label or container for those resources
}



process myProcess4 {...}
// repeat as many process

workflow{
    
    myProcess1()
    myProcess2()
    myProcess3()...}


// even better would be to index some way and loop the process on that index 
// with some if than logic


